{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# plotting\n",
    "%matplotlib inline\n",
    "\n",
    "# data analysis\n",
    "import pandas as pd\n",
    "\n",
    "# data visualization\n",
    "from helper_functions import plot_setup\n",
    "plot_setup()\n",
    "\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the final example of a supervised machine learning model, we'll look at **K-Nearest Neigbor Classifier** and how changing the number of neighbors and the weighting scheme influences model performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, load the same dataset as in the previous examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "titanic = pd.read_csv('titanic_processed.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X = titanic.drop('survived', axis = 1)\n",
    "y = titanic['survived']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### K-Nearest Neighbor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "K-nearest neighbor is a pretty cool algorithm, which is different from the ones we've learned so far because it doesn't learn parameters. There are no coefficients or parameters learned by this algorithm. Instead it keeps track of the data and queries the data during classification time.\n",
    "\n",
    "It's a fairly simple algorithm. You keep track of all of the data points, plotted in their multidimensional space. You already know which classes these data points belong to because you have their labels.\n",
    "\n",
    "When a new data point comes in and you want to classify it, you plot that in the multidimensional space along with all of the other data points.\n",
    "\n",
    "To find the best class for the point, you look at the labels for the k-nearest neighbors (the k data points closest to the new data point). You return the most common class amongst the k-nearest neighbors as the new point's label.\n",
    "\n",
    "You can play around with several model input parameters to change your classifier:\n",
    "\n",
    "* How many neighbors you look at (k).\n",
    "\n",
    "* How you weight the distance of a neighbor when considering it's class. For example, if one data point is really close to your new point, you probably expect that class to be more likely than a bunch of data points much further away from your point.\n",
    "\n",
    "* The type of function you use to measure distance (look up [Euclidean distance](https://www.cut-the-knot.org/pythagoras/DistanceFormula.shtml) or [Manhattan distance](https://en.wiktionary.org/wiki/Manhattan_distance) as a few options if you're interested)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll initialize three different types of models to see how the number of neighbors changes the model.\n",
    "\n",
    "Quickly note that we use the `uniform` weighting scheme here. That means that all of the neighbors contribute equally to the class calculation, regardless of their distance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "models = [('2 Neighbors Uniform Weights', KNeighborsClassifier(n_neighbors=2, weights='uniform')),\n",
    "          ('5 Neighbors Uniform Weights', KNeighborsClassifier(n_neighbors=5, weights='uniform')),\n",
    "          ('15 Neighbors Uniform Weights', KNeighborsClassifier(n_neighbors=15, weights='uniform'))]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll start with four features that we have found to be predictive (pclass, gender, age, family_status)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X_sub = X[['pclass', 'gender', 'age', 'family_status']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Separate the data into training and test sets using a 80/20 split."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X_sub, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's train, predict, and see the accuracy score for each of our models.\n",
    "\n",
    "Which model has the highest accuracy?<br />\n",
    "Why do you think this may be the case?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for name, model in models:\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_test)\n",
    "    print(name + ' Accuracy: ' + str(accuracy_score(y_pred, y_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now it's time to explore the effect of the weighting scheme.\n",
    "\n",
    "We use `weights='distance'` here. The `distance` weighting scheme takes the distance into consideration when computing the majority class for the new data point. The neighbors are weighted inversely relating to their distance. Points which are closer to the new point count more than points which are further. More info [here](http://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KNeighborsClassifier.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "models = [('2 Neighbors Distance Weights', KNeighborsClassifier(n_neighbors = 2, weights = 'distance')),\n",
    "          ('5 Neighbors Distance Weights', KNeighborsClassifier(n_neighbors = 5, weights = 'distance')),\n",
    "          ('15 Neighbors Distance Weights', KNeighborsClassifier(n_neighbors = 15, weights = 'distance'))]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll train, predict, compute the accuracy, and plot the models for each of these models.\n",
    "\n",
    "How do the accuracies compare with the `uniform` weight K Nearest Neighbors models from before?\n",
    "Why do you think that's the case?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for name, model in models:\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_test)\n",
    "    print(name + ' Accuracy: ' + str(accuracy_score(y_pred, y_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### How to pick the values for the model parameters?\n",
    "\n",
    "Often the model you'd want to use has a number of parameters you can tweak. Finding the set of parameters that work best in your particular case can be time consuming and often bears the risk of introducing overfitting. `scikit-learn` provides a way for you to search for the best parameters automatically and in a cross-validated way."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "parameters = {'n_neighbors': [2, 3, 5, 7, 9, 11, 13, 15], 'weights': ['uniform', 'distance']}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "clf = GridSearchCV(KNeighborsClassifier(), parameters, cv = 5)\n",
    "clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is now the best accuracy score we have after the grid search."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "clf.best_score_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And these are the parameters that the search has found to have the highest accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "clf.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to train the model on our data, `clf.best_estimator_` now becomes our model, and everything else looks exactly like in the previous examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = clf.best_estimator_\n",
    "model.fit(X_train, y_train)\n",
    "y_pred = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "accuracy_score(y_pred, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This accuracy score is lower than our best score above even though we've used the best parameters. Why do you think that is?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**On Your Own**: Play around with the `p` input parameter. It is called the \"Power parameter for the Minkowski metric\", but just think about it as the metric used to measure distance between two points.\n",
    "\n",
    "`p=1`: Manhattan distance  \n",
    "`p=2`: Euclidean distance  \n",
    "\n",
    "How does changing the value affect accuracy?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
