{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# plotting\n",
    "%matplotlib inline\n",
    "\n",
    "# data analysis\n",
    "import pandas as pd\n",
    "\n",
    "# data visualization\n",
    "from helper_functions import plot_setup\n",
    "plot_setup()\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this part we'll look at **Support Vector Machine** and how it performs compared with the previous model we trained (logistic regression). \n",
    "\n",
    "At the end, we'll introduce and discuss a concept called cross validation, which allows us to better evaluate our model and avoid a common problem known as model overfitting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, we load the processed Titanic dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "titanic = pd.read_csv('titanic_processed.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X = titanic.drop('survived', axis = 1)\n",
    "y = titanic['survived']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Support Vector Machine"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Support Vector Machine is a complicated title for a fairly simple concept.\n",
    "\n",
    "It comes down to finding a hyperplane which separates our items into two classes (hyperplane as opposed to line/curve because we can work in a number of dimensions - the number of dimensions depends on the number of features). Look below for how we use this to classify more than 2 classes.\n",
    "\n",
    "The kernel (or function) specifies which shape of hyperplane is used. We will at first use a linear kernel, which means we are linearly separating our items for classification.\n",
    "\n",
    "---\n",
    "\n",
    "This might help you envision this on a 2-dimensional level:\n",
    "\n",
    "Imagine an X-Y plane, with a bunch of dots representing data points scattered across the plan. In this X-Y plane, we only have dots which belong to one of two classes. We want to separate the dots by their class and we plan to use a line to do that. On one side of the line, we should have all the dots representing one class and on the other we should have the other class.\n",
    "\n",
    "Small problem, though, our data isn't that simple to split. We can't find a line which puts all dots from one class on one side and all from another class on the other side. Some of our dots are mixed in with dots from the other class.\n",
    "\n",
    "That's fine. Our model just tries to do its best and pick the best line which breaks up the dots as best as possible into two classes. Some dots will be misclassified, but as long as most aren't, we're still doing well. We also want to make sure that line is generalizable enough.\n",
    "\n",
    "---\n",
    "\n",
    "Let's import the [SVM model](http://scikit-learn.org/stable/modules/generated/sklearn.svm.SVC.html) from scikit-learn and see how it performs on our data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "\n",
    "model = SVC(kernel='linear')\n",
    "\n",
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our model with `gender`, `pclass`, and `age` performed best in the previous example, so let's start with that one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X_sub = X[['pclass', 'gender', 'age']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Separate the data into training and test sets using a 80/20 split."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X_sub, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train the model and predict the labels for this new model.<br />\n",
    "\n",
    "(Note: All of our code so far, except `model = SVC(kernel='linear')` has been identical to the code for Logistic Regression. That's the great thing about using `scikit-learn'. It is easy to swap out models, so you can focus on what the right model is, as opposed to the implementation.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model.fit(X_train, y_train)\n",
    "y_pred = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What is the accuracy of this new model?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "accuracy_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# data visualization\n",
    "from helper_functions import plot_confusion_matrix\n",
    "\n",
    "plot_confusion_matrix(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What is the accuracy of the support vector machine classifier after we add family status as a feature?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# TODO - train/test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# TODO - train\n",
    "# TODO - predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# TODO - accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# TODO - plot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How does support vector machine model do compared to logistic regression for the same number of features?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now try to use a different kernel for our SVM. As you can imagine, it is not likely that a hyperplane (rather than a hypersurface) would do well in separating the data. We will now try to use a polynomial kernel instead of a linear one in the previous example and see if that helps our accuracy score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = SVC(kernel = 'poly', degree = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# TODO - select the features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# TODO - train/test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# TODO - train\n",
    "# TODO - predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# TODO - accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# TODO - plot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see in the scikit-learn documentation, SVM model has a parameter C -- penalty parameter C of the error term, also called a regularization parameter -- that you can tweak in your model. \n",
    "\n",
    "If you choose a large C, the optimization will choose a smaller-margin hyperplane that does a better job of getting all training points classified correctly. On the other hand, a small C will result in a larger-margin hyperplane, even if that hyperplane misclassifies more points.\n",
    "\n",
    "You can play with the value of C and see how if affects your model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#TODO - change value of C (e.g. from 10^[-1, 1]) and observe if and how the accuracy changes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Overfitting and Cross Validation\n",
    "\n",
    "There's a common problem that happens when building models. It's called _overfitting_.\n",
    "\n",
    "The goal of a model is to learn generalizations from a training set of data and use these generalizations to predict future unknown examples.\n",
    "\n",
    "Our generalizations are only as good as the diversity of our training set/previous examples. Overfitting happens when you learn too much from a specific set of examples, in a way that makes them worse generalizations for unseen data.\n",
    "\n",
    "This can easily happen if we always use the same training and test sets. We can get really good at learning exactly the right coefficients and parameters for our models, so that we always do well on our test set. Then when we get unseen data in the future, we perform terribly, because the data are not similar to the test set we worked with.\n",
    "\n",
    "This is where cross validation can come in.\n",
    "\n",
    "With cross validation, we use the same data, but work with several train/test splits of that data. We average the accuracy across all runs with these different train/test splits and use that as the accuracy of our model. By using different splits, we get a more diverse set of test sets and it is more likely that our learned model can handle more diverse examples in the future.\n",
    "\n",
    "Cross validation allows us to make more use of the data we have, and create better generalizing models, without having to collect significantly more data (though of course, the more examples you have, the better!)\n",
    "\n",
    "We use the `cross_val_score` to compute the cross-validation accuracy score of model. The `cv` parameter determines cross-validation splitting strategy. For an integer (N), it splits the data into N training and test sets and determines the accuracy score for each of them. N can be as large as the number of your samples; when N is equal to the number of samples we call that _leave one out_. In each step of leave one out cross validation we only have 1 data point in our test set, and all but one data points in the training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "cross_val_score(model, X_sub, y, cv=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using a mean of cross validated accuracy scores provides a better indicator of how our model is performing, and whether the tweaks we're making are improving the model or just overfitting it to the training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "cross_val_score(model, X_sub, y, cv=5).mean()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
