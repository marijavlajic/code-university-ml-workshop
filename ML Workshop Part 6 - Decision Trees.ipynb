{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# plotting\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "\n",
    "# data visualization\n",
    "import seaborn as sns\n",
    "from helper_functions import plot_setup, plot_confusion_matrix\n",
    "sns.set_style('white')\n",
    "plot_setup()\n",
    "\n",
    "# data analysis\n",
    "import pandas as pd\n",
    "\n",
    "# data mining & ML\n",
    "from sklearn import preprocessing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this part, we'll look at Decision Trees.\n",
    "\n",
    "First, let's load the Titanic dataset previously prepared."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "titanic = pd.read_csv('titanic_processed.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's extract the feature and label vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = titanic.drop('survived', axis = 1)\n",
    "y = titanic['survived']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decision Trees"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Decision trees are can be one of the easiest ML algorithms to think about conceptually.\n",
    "\n",
    "We'll learn about this concept by jumping into code and exploring the idea."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Gender Feature\n",
    "\n",
    "In our first notebook, during data exploration, we noticed that gender was a signifcant factor in predicting survival. Let's look at that again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display counts of survivors for each gender category\n",
    "sns.countplot(data = titanic, x = 'gender', hue = 'survived')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that gender is a significant factor. So what would happen if we just used gender to predict if an individual survives?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here we predict that if you are a woman you survive, if you are not, you won't \n",
    "\n",
    "def predict(features):\n",
    "    if features['gender'] == 0:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How well does this simple predictor perform?\n",
    "\n",
    "Let's pull out a test set using `train_test_split`. We won't be using the train set for now since our predictor model is already built based just on gender."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Create a data frame from the train data to use in our simple predictor\n",
    "# This is the data that we will analyze (we never want to look at our test data\n",
    "# or change our model based on it)\n",
    "train_df = X_train.copy()\n",
    "train_df['survived'] = y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = [predict(row) for _, row in X_test.iterrows() ]\n",
    "accuracy_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's not a bad start. We of course know that this is too simple a classifier, though.\n",
    "\n",
    "Let's get a bit more specific. Age seemed to play a role in survival. Let's look at age by gender and see if we can improve our model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Age**\n",
    "\n",
    "Women's survival by age:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.distplot(a = train_df['age'][(train_df['survived'] == 1) & (train_df['gender'] == 0)].dropna(), kde_kws = {'label': 'survived'})\n",
    "sns.distplot(a = train_df['age'][(train_df['survived'] == 0) & (train_df['gender'] == 0)].dropna(), kde_kws = {'label': 'did not survive'})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Men's survival by age:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.distplot(a = train_df['age'][(train_df['survived'] == 1) & (train_df['gender'] == 1)].dropna(), kde_kws = {'label': 'survived'})\n",
    "sns.distplot(a = train_df['age'][(train_df['survived'] == 0) & (train_df['gender'] == 1)].dropna(), kde_kws = {'label': 'did not survive'})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For men, there seems to be a much higher rate of survival if they were younger than 15. We can add that to our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(features):\n",
    "    # TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How accurate is our predict function for our test data?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "y_pred = [ predict(row) for _, row in X_test.iterrows() ]\n",
    "accuracy_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see almost increase in 1.5% accuracy from also distinguishing by age."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We would like to better distinguish and predict for women. Right now we are saying that all women survived. That isn't true and it is affecting the accuracy of our predictions.\n",
    "\n",
    "Let's explore some other features that we might use to better predict for female passengers.\n",
    "\n",
    "**Ticket Class**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Survival rates for women based on their ticket class\n",
    "sns.countplot(data = titanic[titanic[\"gender\"] == 0], x = 'pclass', hue = 'survived')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Almost all women with 1st and 2nd class tickets survived, but many women with 3rd class tickets did not. Let's include this in our prediction function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(features):\n",
    "    # TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How accurate are our predictions now?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = [ predict(row) for _, row in X_test.iterrows() ]\n",
    "accuracy_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that our accuracy went down slightly. This means that the last break down didn't have great information gain (more below)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On Your Own: Keep playing around with data break downs. What combination of conditions mean that a passenger was likely to survive or not survive?\n",
    "\n",
    "You can continue to expand on the `predict` function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Back to Decision Trees\n",
    "\n",
    "What we did above is exactly the beginnings of a decision tree.\n",
    "\n",
    "A decision tree is a logical tree structure, which asks questions at each branch. Based on the answer to that question, you either go right or left from that branch. You proceed down the logic tree until you get to a node which gives you the predicted value.\n",
    "\n",
    "You get a new data point. A new passenger, for example.\n",
    "You ask a series of questions about the passenger and after you answer all of those questions, you have a prediction/answer about if the passenger survived or not.\n",
    "\n",
    "**Important Concepts:**   \n",
    "*Entropy*   \n",
    "An important part of forming a decision tree is something called [entropy](https://en.wikipedia.org/wiki/Entropy_(information_theory)).  \n",
    "Entropy is essentially a measure of how varied (or random) the values are in a set. A set of items where all items are identical, has low entropy. A set of items where every item is different from the others, has high entropy.\n",
    "\n",
    "When creating decision trees, the goal is to reduce the amount of entropy in your resulting data set at each step. You want to find questions which help separate your data into as many clear sets as possible.\n",
    "\n",
    "We are predicting survival, either 1 - survived or 0 - did not survive. Every question we ask splits our data up into some part. In each data split, we want as little entropy in the resulting set of data as possible.\n",
    "\n",
    "Our ideal question would split surviving passengers from passengers who did not survive. Unfortunately we don't have any one question like that, but with a series of questions we can get closer and closer to that goal.\n",
    "\n",
    "*Information Gain*  \n",
    "Information gain refers to the change in differentness/randomness in your data set, before and after your conditional branch (i.e. before and after you asked your question). A very good condition in your decision tree, will give you a high information gain. In our example, that means it would do a great job splitting passengers who survived from passengers who did not.\n",
    "\n",
    "A bad condition, would provide us with almost no more information after the split. We would have almost no more information about if a passenger survived or did not.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Decision Tree Classifier with Sklearn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We don't need to create a decision tree manually, though. Scikit learn has tools already which will automatically generate a logic tree for us and use it to predict new data. Let's see how it performs.\n",
    "\n",
    "[Scikit Learn Decision Tree Classifier](http://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html#sklearn.tree.DecisionTreeClassifier)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn import tree\n",
    "\n",
    "clf = tree.DecisionTreeClassifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "clf.fit(X_train, y_train)\n",
    "y_pred = clf.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_confusion_matrix(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lastly, let's visualize our decision tree classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import graphviz \n",
    "dot_data = tree.export_graphviz(clf, out_file=None) \n",
    "graph = graphviz.Source(dot_data) \n",
    "graph "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "This graph shows us a very important point about decision tree classifiers. The automatically generated ones can become incredibly complicated, and the more complicated the are, the more likely it is they are overfitted or highly specialized around our training data.\n",
    "\n",
    "This could be why our sklearn decision tree underperformed our manually determined one. This is something to keep in mind when working with decision trees."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are some parameters of our model we can change to reduce overfitting/the complexity of our model.\n",
    "\n",
    "**On Your Own:**\n",
    "Play around with the `max_depth` and `min_samples_split` fields. See how they change your resulting visualized decision tree."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
